<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V2.0//EN"
"http://forrest.apache.org/dtd/document-v20.dtd">
<document xml:lang="en">
  <header>
    <title>Building a preprocessor for the Giellatekno/Divvun languages</title>
  </header>

  <body>
      <p>The preprocessor file is common to all languages. It should
      be parametrised, certain languages, such as Skolt Saami and
      Haida, have very deviant orthographic conventions when it comes
      to punctuation marks.</p>

    <section>
      <title>Our forthcoming preprocess method: Using Hfst as a preprocessor</title>
      <p>Using Hfst, one can tokenise («preprocess»), analyse and print the
        output in VISLCG3 format, all in one go, and everything using a single
        transducer. This will handle multiword expressions properly, including
        all inflections of them. This setup replaces the older, Perl-based
        solution for the Xerox tools.
      </p>
      <p>The new command is as follows:</p>
      <source>cat testfile.txt | hfst-tokenise --giella-cg tools/tokenisers/tokeniser-disamb-gt-desc.pmhfst| vislcg3 -g tools/tokenisers/mwe-dis.cg3 | cg-mwesplit | vislcg3 -g src/syntax/disambiguator.cg3</source>

    </section>


      <section>
        <title>Overview and intro</title>

        <p>This document describes how the preprocessing of the text into
        separate words is implemented in the project. The document contains
        overall description of the preprocessing task and some implementation
        details.</p>

        <p>The preprocessor is a Perl script called <code>preprocess</code>, it
        is located in <code>$GTHOME/gt/script/</code>. It takes
        text as input and produces a list of words (tokens) separated by
        newlines. The preprocessor has two main functions: it cuts the the text
        into sentences and sentences into tokens (words and other units such
        as numerical expressions and punctuation). Sentence delimiters and
        most of the punctuation are treated as separate tokens.</p>

        <p>The output of the preprocessor is a list of tokens separated by
        newline. The list of tokens is input to the morphological parser. (See
        <a href="global-flowchart.html">the flowchart</a> of the parsing
        process; if you are using Hfst (see above), the flowchart is outdated:
        all steps up to the vislcg3 step is collapsed into one.) The parser
        gives each token a morphological analysis, a set
        of tags. For the analysis to be succesful, the preprocessor must be
        compatible with the parser; the preprocessor should produce tokens
        that are recognized by the parser. This means that for example
        multiword expressions like "earret eará" have to be recognized as a
        single token (not as two separate words) both by the preprocessor and
        the parser. To achieve this, the lexicon entries of "special" tokens,
        such as punctuation, abbreviations and multiword expressions are
        planned parallel with the preprocessor (see the section <a
        href="#lex">Abbreviations and the lexical files</a>).</p>
      </section>

      <section>
        <title>Dividing text to sentences</title>

        <p>The text is divided to sentences according to sentence-delimiters
        (?,! and .); the question mark and exlamation mark are always
        sentence-delimiters, but each full stop is examined separately. Full
        stop may belong to an abbreviation that may or may not end the
        sentence. Similarly, a full stop that ends a numeral expression is
        sentence-delimiter only if the next word starts with a capital
        letter.</p>

        <p>When a full stop that belongs to an abbreviation can be considered
        to be a sentence delimiter as well, an extra full stop is added to
        mark the sentence boundary. The sentence boundaries are relevant
        information later in CG. For example:</p>

        <source>Vuođđologut (lohkosánit) leat: okta, guokte, golbma jna. Sulliilogu sánit leat: moadde, máŋga jna.
Vuođđologut
(
lohkosánit
)
leat
:
okta
,
guokte
,
golbma
jna.
.
Sulliilogu
sánit
leat
:
moadde
,
máŋga
jna.
.</source>

        <p>The abbreviations are divided to three types according to their
        behaviour with other words. Some abbreviations which are followed by a
        capitalized word cannot be considered to end a sentence, see
        abbreviation section.</p>

        <p>The corpus database used in the project contains additional
        paragraph marking and the end of paragraph is marked with ¶ in the
        input to the preprocessor. The paragraph end is at the same time a
        sentence end and makes it possible to consider e.g. titles as
        "sentences". The paragraph end mark sometimes adds an extra dot to the
        preprocessor output, sometimes it is recognized only later in the CG,
        these functions should be unified.</p>
      </section>

      <section>
        <title>Tokens</title>

        <p>The string that is given to the preprocessor is divided into tokens,
        which are the basic processing units used in the lexical analysis.
        There are four types of tokens: words, multiword expressions,
        numerical expressions and punctuation. Numerical expressions consist
        of numbers, dates, time, prices, etc. Words include abbreviations. The
        punctuation that does not belong to any of the mentioned expressions,
        forms its own class, which includes for example {}[]()?!;"</p>

        <p>The starting point is that a string surrounded by space is one
        token. A string that consist only of alphabetical characters
        surrounded by space is always an instance of one word token, if it
        does not belong to a multiword expressions. So if two words are
        accidentally typed without separating space, it is an error that's not
        accounted for. Word tokens may contain some punctuation, such as
        hyphen: <em>Davvi-Norga. </em></p>

        <p>The punctuation that occurs in the text is divided to two classes:
        1. those that are inseparable part of a word or numeric expression and
        2. those that are independent. Independent punctuation may split the
        input string into two or more parts. For example the string
        <em>(gielddat/guovllut).</em> will be divided into six parts: both
        parentheses are their own tokens as well as the dot. Both words and
        the slash are also separate tokens. The preprocessor output is
        thus:</p>

        <source>(
gielddat
/
guovllut
)
.
</source>

        <p>The splitting depends on the surrounding characters. For example, a
        dot rarely splits a numerical expression into two but generally splits
        a string that consist only of alphabetical characters (<em>1.4.2004,
        Minä lähdin.En tule takaisin.</em>). The treatment of <a
        href="#punctuation">punctuation</a> is explained in detail later in
        the document.</p>
      </section>

      <section>
        <title>The preprocess script</title>

        <p>The preprocessor is a perl-script called <code>preprocess</code>;
        it reads <code>STDIN</code> for input. The preprocessor is given one
        command line parameter, the file from where to read the formerly
        generated list of abbreviations and multiword expressions. The file is
        created by <code>make</code> and the name is <code>abbr.txt</code>. Usage
        (with sme as an example):</p>

        <source>preprocess --abbr=$GTCORE/langs/sme/src/syntax/abbr.txt
</source>

        <p>The output of the script is a list of analyzable tokens, separated
        by newline.</p>

        <p>The text is handled one paragraph at a time (paragraphs end when
        two subsequent newlines are encountered). If there are no paragraphs
        in the text, the whole input is slurped in in one go. The newlines are
        replaced with space and the paragraph is treated as a single line. The
        line is divided using space (tab, newline) as a separator and the
        elements stored into an array. Example:</p>

        <source>First read the text. (Then split by space.)

First
read
the
text.
(Then
split
by
space.)
</source>

        <p>The elements in the array are processed one at the time. The
        element may consist of two or more tokens, the tokens are stored in an
        array. First, the punctuation preceding and not belonging to the token
        is removed and stored to the tokens array. This class of punctuation
        includes <code>{}[]()?!;,'\"</code>. If the remaining part starts with
        an alphabetical character optionally preceded by one non-word
        character, it is considered to be a word token. Otherwise it is a
        numerical expression.</p>

        <p>The word tokens are tested against the list of multiword
        expressions. The punctuation at the end of the word that does not
        belong to the token is stored to the tokens array. The word may be
        divided to several tokens if it contains delimiters such as
        <code>/</code>.</p>

        <p>If the word contains a dot, it is tested against the list of <a
        href="#abbreviations">abbreviations</a> to see if the dot is a
        sentence delimiter or belongs to the token. The treatment of the
        numerical expressions differ in that most of the punctuation, such as
        <code>/</code>, is considered to be part of the token. The dot that
        follows a number is considered to be a sentence delimiter if the
        following word starts with a capital letter. Otherwise the dot belongs
        to the token which may be an ordinal number, date etc.</p>

        <p>There are some constant variables that affect to the preprocessing,
        here are the current values:</p>

        <source>$MULTIWORD_SIZE = 3;
$SINGLE_PUNCT contains:  «»‹›“”„‘’‚ ´`'" |{}[]()?!,;/\ … and • ¶
$CONTAIN_PUNCT = 'ja\/dahje|http|:\/\/|km\/h|www|@|\.jpg|\.doc|\.pdf|\.html|\.txt';
</source>

        <p><code>$MULTIWORD_SIZE</code> defines the size of a multiword
        expression, i.e. how many words should be included in testing if the
        expression is a multiword. The constant <code>$SINGLE_PUNCT</code>
        contains all the punctuation that is considered to be individual
        tokens in the beginning or end of a word or numeral expression. In
        non-numeral expression, the punctuation in variable $SINGLE_PUNCT is
        searched also in the middle of the word, thus splitting it
        ("gielddat/guovllut"). The word tokens that contain punctuation which
        belongs to the token are defined in the constant
        <code>$CONTAIN_PUNCT</code>. This constant is currently the only
        language dependent part of the script.</p>
      </section>

      <section>
        <title>Punctuation</title>

        <p>As most of the punctuation in a numerical expression is inseparable
        part of it, the numerical expressions are treated as a separate class.
        The punctuation in other strings generally causes the string to be
        splitted, but not always, depending on the punctuation mark. In the
        following table, the punctuation marks are listed:</p>

        <table>
          <tr>
            <td>Punctuation</td>

            <td>Numerical expressions (digits and non-word characters)</td>

            <td>Words (word characters, no digits)</td>
          </tr>

          <tr>
            <td>(){}[]– «»‹›“”„‘’‚ ´`'"?!,;\… • ¶</td>

            <td>Always their own tokens.</td>

            <td>Always their own tokens.</td>
          </tr>

          <tr>
            <td>:</td>

            <td>Belongs to the expression if not followed by space (14:30,
            10:s)</td>

            <td>Belongs to the expression if not followed by space
            (Namdal:as)</td>
          </tr>

          <tr>
            <td>/</td>

            <td>Belongs to the expression (5/2004)</td>

            <td>Always its own token (gielddat/guovllut). Exceptions: ja/dahje
            which is treated as one token; html-addresses, file names
            etc.</td>
          </tr>

          <tr>
            <td>-, --</td>

            <td>Belongs to the expression, also when surrounded by spaces
            (1-3, 1 - 3)</td>

            <td>Belongs to the expression (Davvi-Norga), but not in the
            beginning of the word.</td>
          </tr>

          <tr>
            <td>.</td>

            <td>Belongs to the expression unless it is a sentence
            delimiter.</td>

            <td>Its own token. Exception: belongs to an abbreviation and does
            not end the sentence.</td>
          </tr>

          <tr>
            <td>_*+=</td>

            <td>Belongs to the expression</td>

            <td>Always its own token</td>
          </tr>

          <tr>
            <td>%</td>

            <td>Belongs to the expression, also when separated by space (50%,
            50 %)</td>

            <td>Always its own token</td>
          </tr>
        </table>
      </section>

      <section>
        <title>Abbreviations</title>

        <p>The abbreviations are divided in three classes according to whether
        they are able to end the sentence or not. The first class contains
        transitive abbreviations (<code>TRAB</code>) that never end the
        sentence. The intransitive abbreviations (<code>ITRAB</code>) are
        considered to end the sentence whenever they are followed by a capital
        letter or number. The third class (<code>TRNUMAB</code>) contains
        abbreviations that end the sentence when followed by a capital letter
        but not when followed by number. In addition, there is a class for
        abbreviations that are homonyms for common nouns (<code>NOAB</code>),
        these expressions are treated as abbreviations only when they are in
        the middle of the sentence, that is, when followed by lowercased
        letter or punctuation. In sum:</p>

        <dl>
          <dt>TRAB</dt>

          <dd>abbreviations that take an object: 'Mr. Peters'</dd>

          <dt>ITRAB</dt>

          <dd>abbreviations that do not take an object: 'Lloyds Ltd.'</dd>

          <dt>TRNUMAB</dt>

          <dd>abbreviations that take NUM objects: 'Downing Str. 10') There is
          sentence boundary after TRNUMAB only for the capital+small
          combinations and all small-initial strings that consist of more than
          one letter. For all other strings (one small letter, one or several
          capital letters) there is no sentence boundary after TRNUMAB.</dd>
        </dl>

        <dl>
          <dt>NOAB</dt>

          <dd>abbreviations such as 'du.' (dual) which is also a common noun
          and not treated as an abbreviation at the end of a sentence.</dd>
        </dl>

      <p>To make the preprocessor handle abbreviations and multiword
      expressions better than the default, one can provide an abbreviation
      file to it. There is a Makefile target for that. This target extracts
      abbreviations and multiword expressions
      from the lexc code, classifies abbreviations according to
      behaviour in the sentence, and expands the multiword expressions
      in all cases.</p>

      <p>To build the abbreviation list to be used by <code>preprocess</code>,
      do as follows:</p>
      <source>./configure --enable-abbr
cd tools/tokenisers
make abbr</source>

      <p>Now there is a new <code>abbr.txt</code> file in that dir, and you
      can specify it as an option to the
      <code>preprocess</code> command. See further notes above.</p>

      </section>

      <section>
        <title>Multiword expressions</title>

        <p>The preprocessor takes into account multiword expressions if they
        are listed in the file <code>abbr.txt.</code> For example, names such
        as 'De Silva' can be listed as multiword expressions, they are not
        splitted but treated as tokens. At the moment, the size of a multiword
        expressions may be three words. </p>
      </section>

      <section>
        <title>Listing abbreviations and multiword expressions</title>

        <p>All the abbreviations are extracted from the lexicon files. They
        are extracted from the file as a part of the compilation process
        <code>make</code>. The relevant commands are stored to the Makefile in
        the <code>gt/lang/src</code> directory. The script that handles the
        extraction is called <code>abbr-extract.pl</code> and located in the
        <code>gt/script</code> directory; it is intended to be a
        language-independent script.</p>

        <p>The main source is <code>abbreviations.lexc</code>, where the real
        abbreviations are listed. There also exists a number of multi-word
        expressions which are dispersed to different lexicon files, for
        example the file <code>adverbs.lexc</code> contains multi-word
        adverbials like <code>earret eará</code>. Not to mention the
        propernoun lexicon, which contains several multiword
        propernouns.</p>

        <p>In order to be able to treat the multi-word expressions as single
        tokens already at the preprocessing phase, the multi-word expressions
        are extracted from the lexicon files. The relevant lexicon file names
        are given to the script <code>$GTHOME/gt/script/abbr-extract</code> by the
        Makefile (NOTE: NOT IMPLEMENTED YET, dec 2013). There are no restrictions to how to
        list the multiword expressions, they can be used in lexicon files as
        other tokens. Only the structure of the file
        <code>abbreviations.lexc</code> is restricted.</p>

        <p>Usage of the script <code>abbr-extract</code>:</p>

        <source>abbr-extract --output=&lt;file&gt; --abbr_lex=&lt;file&gt; --lex=&lt;file1,file2,..&gt;</source>

        <section>
          <title>The structure of abbreviations.lexc</title>

          <p>The structure of the <code>abbreviations.lexc</code> is free in
          all the other respects but the extracted part must have the
          following syntax:</p>

          <source>LEXICON ITRAB
abbr  TAG ;
ab.br  TAG ;
ab  TAG ;
abbr% abbr TAG ;
</source>

          <p>And the same to the other abbreviation classes, <code>LEXICON
          TRAB, LEXICON TRNUMAB</code> and <code>LEXICON NOAB</code>. The
          ordering of the abbreviation classes is free.</p>

          <p>Due to xfst, abbreviation may not contain dots (operator in
          xfst). The pattern matching in the preprocessor is always done
          without the dot. It is possible to add the dot to the abbreviation
          again in the lex-file.</p>

          <p>The space in multiword abbreviations is marked in the lexicons by
          the %-sign (literal operator) which is recognized by
          <code>abbr-extract</code>-script and removed.</p>

          <p>Abbreviations in <code>abbreviations.lexc</code> are lower-cased
          unless there exists only upper cased version of the abbreviation.
          <code>abbr-extract</code>-script generates upper case versions of
          all the abbreviations (since they may begin the sentence).</p>
        </section>
      </section>

      <section>
        <title>Other</title>

        <section>
          <title>XML-tags</title>

          <p>There is a primitive XML-support in the preprocessor. XML-tags
          are recognized and they can be excluded or included to the output.
          Choose the option --xml to accept xml-formatted input. Each XML-tag
          is printed on its own line. With option --no-xml-out, the XML-tags
          are supprsessed. The XML-support is called 'primitive', since the
          parsing of XML-tags is not done using any library functions.
          Therefore, the XML-formatting must be quite clear to function.</p>
        </section>

        <section>
          <title>Spaces</title>

          <p>It is possible to preserve space during the tokenization with
          optin --space. The space between tokens is preserved and printed in
          their own lines, for example:</p>

          <source>    leat: okta, guokte, golbma jna.
    leat
    :
    &lt; &gt;
    okta
    ,
    &lt; &gt;
    guokte,
    &lt; &gt;
    golbma
    &lt; &gt;
    jna.</source>

          <p>It is also possible to preserve newlines in the output, there is
          a variable in the script that can be changed. If the option --xml is
          used together with space preserving, also the space between xml-tags
          is preserved. On the other hand, if the option --no-xml-out is used,
          the space that precedes the xml-tag is swallowed.</p>

          <p>Space preservation is implemented so that each token forms a
          complex unit: the word is paired with the space that preceded it.
          The space is preserved also when tokens are combined to multiword
          expressions.</p>
        </section>

        <section>
          <title>Typos-list</title>

          <p>In some cases, we want to fix the common typos in the texts. The
          list can be provided to preprocessor with option
          --corr=&lt;file&gt;. Typos are a tab separated list:</p>

          <source>     typo&lt;TAB&gt;correction
     typo2&lt;TAB&gt;correction</source>

          <p>Each typo is replaced in the text by its correction. Typos and
          their corrections may be multiword expressions.</p>
        </section>

        <section>
          <title>Connecting expressions</title>

          <p>There is a class of expressions that connect other expressions to
          form complex tokens. This is for example expressions like 'fisk- og
          vilthandelen'. These kind of expressions are generic and cannot be
          listed in multiword expressions. At the moment, specifically this
          type of complex token is handled with option
          --connect=&lt;exp1,exp2,..&gt;, where the words that are used as
          connectives can be listed.</p>
        </section>
      </section>

      <section>
        <title>Possible flaws</title>

        <p>Abbreviations may be followed by capital letter even when not
        ending the sentence</p>

        <source>Siellä oli Pekka ym. Mäkisiä.

Siellä
oli
Pekka
ym
.
Mäkisiä
.

Siellä oli Pekka ym. Mäkinen ei tullut.

ym
.
Mäkinen
</source>
      </section>

      
      $Author$</p>

  </body>
</document>
