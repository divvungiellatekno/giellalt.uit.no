<html xmlns:xi="http://www.w3.org/2001/XInclude" lang="en">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Technical documentation of the corpus conversion process</title>
   </head>
   <body>
      <h1>Conversion process</h1>
      <p><a href="http://www.webhostinghub.com/support/es/misc/documentacion-tecnica">This text in Spanish ~ Este texto en español (translation: Maria Ramos)</a></p>
      <p>This document describes the more technical side of the corpus
         infrastructure. The document <a href="corpus_conversion.html">
            		  corpus_conversion.html </a> contains the
         user documentation as well as the description of the basic
         infrastructure. The usage of the script convert2xml.pl is documented
         there, this document describes the contents of the script.
      </p>
      <p>All the binaries and scripts used in the corpus conversion process
         	  are stored under $GTHOME/gt/script/corpus, except the tools that are
         already installed to some common bin. The xsl-files which transfer the
         structural information of different source files (docbook2xml.xsl and
         xhtml2xml.xsl) and the template for file specific xsl-files
         (XSL-template.xsl) are also stored there. The directory <code>$GTFREE|$GTBOUND/tmp/</code> is reserved for
         temporary files and log files that are created during the conversion.
         The log files contain the system commands executed during the conversion
         as well as the warnings and error messages. The log file is named after
         the file that is converted.
      </p>
      <p>The next sections describe the conversion processes for each document
         type and the tools used. The conversion includes hyphen-detection and
         language recognition as well as decoding the wrongly utf-8 -encoded
         characters.
      </p>
      <h2>Word documents</h2>
      <p>Microsoft Word documents are converted with the program
         <code>antiword</code> to produce a docbook xml, and piped to the xslt
         program xsltproc, that converts to our XML-format. We have an
         xsl-document docbook2corpus.xsl, that is used in converting the
         document.
      </p>
      <h2>RTF documents</h2>
      <p>RTF documents are converted to html using <a href="http://www.gnu.org/software/unrtf/">unrtf</a>, then to our xml format using <a href="https://gtsvn.uit.no/langtech/trunk/gt/script/corpus/xhtml2corpus.xsl">xhtml2xml.xsl</a>.
      </p>
      <h2>Ávvir xml documents</h2>
      <p>These documents are converted to our xml format using <a href="https://gtsvn.uit.no/langtech/trunk/gt/script/corpus/avvir2corpus.xsl">avvir2corpus.xsl</a></p>
      <h2>SVG documents</h2>
      <p>These documents are converted to our xml format using <a href="https://gtsvn.uit.no/langtech/trunk/gt/script/corpus/svg2corpus.xsl">svg2corpus.xsl</a></p>
      <h2>Plain text document</h2>
      <p>Plain text documents that are stored to the database should have
         the extension <code>.txt</code>. The encoding of a text document is
         solved using the iconv -tool. If there are special markings for
         headings and paragraphs, like in some newspaper texts, they are used
         in creating the document structure. Otherwise, the empty lines mark
         paragraph breaks and short lines beginning with numbers are treated as
         titles.
      </p>
      <h2>Adobe document</h2>
      <p>Pdf documents are converted to plain text using <a href="http://poppler.freedesktop.org/">pdftotext</a>, and then converted to our xml format using <a href="https://gtsvn.uit.no/langtech/trunk/gt/script/langTools/Corpus.pm">langTools::Corpus::txt_clean</a></p>
      <h2>Web-document</h2>
      <p>Web documents are first cleaned using the program
         		<a href="http://tidy.sourceforge.net/">HTML Tidy </a>, using several
         command line options.. The output is converted to xml using the
         xsl-file <code>xhtml2xml.xsl</code>.
      </p>
      <h2>Paratext document</h2>
      <p>Paratext is a file format for publishing and interchanging basic
         Scripture texts in multiple languages. It is intended to be used for
         all aspects for Bible layout and publishing. The paratext format is
         based on backslash codes, format called USFM, see e.g <a href="http://confluence.ubs-icap.org/display/USFM/Home">http://confluence.ubs-icap.org/display/USFM/Home</a>
         The paratext files are converted using a Perl script paratext2xml.pl
         which forms basic xml-structure which roughly corresponds our
         corpus.dtd. The files that are added to the corpus repository should
         have the suffix .ptx.
      </p>
      <h1>Hyphenation: add-hyph-tags.pl</h1>
      <pre xml:space="preserve">    Usage: add-hyph-tags.pl [OPTIONS] FILES
    Tag the hyphenation marks.
    Options
    --all            search the whole text for hyphenation points.
                 The default is to search only the end of the lines.
    --infile=&lt;file&gt;  the input file.
    --outfile=&lt;file&gt; output file.
    --help           prints the help text and exit.
</pre>
      <p>The script replaces hyphens in the text with tag &lt;hyph/&gt;. The
         hyphens are searched by default at the end of the line. The option --all
         can be used for replacing the hyphens all over the text. The aim of the
         script is to replace only "real" hyphens, i.e. the ones that mark real
         hyphenation points. The hyphens in e.g. numeric expressions are not
         replaced. The words which precede a hyphen but are not hyphenation
         points such as "ja" in expression "davvi-, julev- ja oarjelsámegillii"
         are taken into account.
      </p>
      <p>The reason why the hyphens are tagged by default only at the line
         ends is the existence of e.g. the following correct hyphenation marks
         which do not mark a hyphenation point. Some of these occur at the line
         ends and cause errors.
      </p>
      <pre xml:space="preserve">teknihkalaš-luonddudieđalaš
Norplus-prográmma
norgga-ruoŧa
dánska-norgalaš
</pre>
      <p>The script takes into account the xml-notation of the file. If the
         last word of a paragraph marked with &lt;/p&gt; ends with a hyphen, the
         next paragraph beginning with &lt;p&gt; is searched for the rest of the
         word. Some text extraction tools such as antiword may create this kind
         of structures. The script also reformats the text by removing white
         space, moving &lt;p&gt;-tags and changing the place of the line
         break.
      </p>
      <h1>Language recognition</h1>
      <p>The newly created xml-document is parsed and the language of
         each paragraph is recognized using the tool
         <code>pytextcat</code>. The tool is described in the document <a href="langrec.html">Language recognition using
            pytextcat</a>. The language recognition tool is not perfect, but
         mostly it gets it right.
      </p>
      <p>The document always has a main language, and only the
         differing languages are marked in the xml-structure. By default,
         all the languages in the language model (there are many) can
         occur in the document and they are taken into account in the
         categorization process. However, since the e.g. the different
         Sámi language easily confuse with each other and Finnish, the
         language recognizion can be restricted to some subset of these
         languages. The document can be explicitely marked as
         monolingual, or multilingual containg text fractions of some of
         the abovementioned languages. You should set these variables in
         the file-specific xsl-file.
      </p>
      <h1>XSLT-scripts</h1>
      <p>The structural information, such as titles and paragraphs, that is
         contained in MS Word of pdf document is preserved in the xml-document.
         The antiword program that is used in converting the Word documents
         produces xml docbook format. That format is further transformed to our
         xml-format, using xsl-script docbook2xml.xsl. The similar script
         xhtml2xml.xsl is used in transforming the structural information in html
         document to our xml-format. Pdf-files are first converted to html and
         the same xsl-script is used.
      </p>
      <p>Word documents may contain metainformation, such as the name of the
         owner of the file, which is preserved as well. The other metainformation
         is added to the xml-file through the file-specific xsl-script. The
         process is explained in the <a href="corpus_conversion.html#XSL-files">usage documentation</a>. The
         file specific xml-file is copied from the file XSL-template.xsl, located
         in corp/bin -directory. It contains variables for adding the
         metainformation. These variables always overrides the metainformation
         coming from the original document. The metainformation recieved from the
         web upload script is stored straight to the file-specific xsl-file, so
         the information can be altered manually.
      </p>
      <p>The script <code>common.xsl</code> contains instructions for building
         the final xml-structure of the corpus file. The structure is validated
         against the document type definition, <a href="http://giellatekno.uit.no/dtd/corpus.dtd">http://giellatekno.uit.no/dtd/corpus.dtd</a>
         common.xsl is included in every file-specific xsl-script.
      </p>
      <p>There is a special script <code>empty.xsl </code>to be used instead
         of common.xsl when the document cannot be converted to xml-structure.
         This can happen for several reasons, but the most common reason is that
         the character encoding in the original document is somehow broken; the
         Saami characters may be missing or there are several character encodings
         used, when the result of the conversion is not satisfactory. The
         document could be removed from the database as well, but e.g. some
         newspaper documents are considered to be part of the distribution.
      </p>
      <h1>Useful scripts for corpus maintainer</h1>
      <p>There are several small scripts for corpus database maintenance and
         cleaning. They reside in <code>gt/script </code>-catalog. The most
         important ones are listed here:
      </p>
      <ul>
         <li><strong>corpus_summarize.sh:
               </strong><br>The script is used for generating summarizing information of the
            corpus files and storing them to cvs, under the catalog
            <code>gt/doc/lang/corp</code>. The script calls the Perl-script <code>
               corpus_summary.pl</code>, which generates the summaries. The file
            <code>corpus-content.xml</code> contains a list of all the files in
            the corpus database and some relevant information like the license and
            size. The files <code>corpus-susummary.xml</code> and
            <code>corpus-summary-YYYY-MM-DD.xml</code> contain the total count of
            the documents as well as the number of the documents in different
            subdirectories. The xml-files are further transformed to forrest
            documentation.
         </li>
         <li><strong>corpus_zip.sh:
               </strong><br>The script copies the free-catalog from the corpus database and
            creates zipped tar-archives of it. An archive is created of both the
            xml files and the files that contain only the extracted text in the
            selected language. The archives are copied to the download area.
         </li>
         <li><strong>corpus_chmod.sh:
               </strong><br>The script is used for correcting the group ids and permissions of
            the corpus files in different catalogs. Running the script requires
            sudo rights.
         </li>
         <li><strong>change_xsl_generic.py:
               </strong><br>
            for i in `find . -name \*.xsl`<br>
            do<br>
            change_xsl_generic.py translator_fn Pirjo translator_ln Paavaniemi <br>
            translator_gender fem translator_nat FI  $i<br>
            done<br>
            
         </li>
         <li><strong>gt2ga.sh:
               </strong><br>Copies the selected files to G5 for analysis, compiles the latest
            versions of the tools and runs the analysis for the whole corpus. The
            analyzed files are stored under ga/lang/genre, the whole genre in one
            file. The script is currently used only in G5.
         </li>
         <li><strong>corpus_fix_meta.sh:
               </strong><br>Some of the corpus directories contain several files with similar
            metainformation, e.g. all the newspaper texts may have the same
            collection information, the name of the newspaper. This script is
            planned for updating several file-specific xsl-files at the same time.
            The directory where the files are searched is given in variable inside
            the script. The script uses xsl-script <code>change_xsl.xsl </code>for
            the transformation. The xsl-script should be modified for the
            different uses and the variable containing the path to the script
            changed. The version control of the xsl-files is handled
            automatically, although sometimes stealing a lock is necessary and
            requires some typing.
         </li>
         <li><strong>corpus_rename.sh:
               </strong><br>The names of the corpus files have to face some basic
            requirements: They should be in utf-8 encoding, in NFC, not contain
            any special characters that would harm the shell ("´? etc.), have
            spaces replaced by underscores or removed and have the correct
            extensions (.txt, .html, .doc, .pdf or .ptx). This script searches
            whole directories for filenames and changed them to confront these
            requirements. If the file already has some extension, it is not
            changed.
         </li>
         <li><strong>corpus_bad_encoding.sh:
               </strong><br>Some of the character set conversions do not succeed well enough
            for the purposes of linguistic analysis. The files may contain
            different encodings in different parts of the file, single
            unrecognized characters or the character set is not recognized at all.
            Those files can be located using this script.
         </li>
         <li><strong>Makefile:
               </strong><br>There is a Makefile in /usr/local/share/corp for converting files
            that are not up-to-date concerning the original or file-specific
            xsl-file. Usage: <code>make LANGUAGE=sme GENRE=facta</code> or
            <code>make path/to/file.xml</code>.
         </li>
      </ul>
      <h1>corpus.dtd</h1>
      <p>corpus.dtd contains the document type definition for the
         xml-structure. It is stored in <a href="http://giellatekno.uit.no/dtd/corpus.dtd">http://giellatekno.uit.no/dtd/corpus.dtd</a>The
         fields are briefly described in the following:
      </p>
      <pre xml:space="preserve">document (header,body)</pre>
      <h2>header</h2>
      <p>The document is divided into to elements: header that contains the
         metainformation and body for the document content. The header contains
         the following fields:
      </p>
      <ul>
         <li><strong><code>title</code>:
               </strong><br>The title of the document
         </li>
         <li><strong><code>genre</code>:
               </strong><br>The document genre can be one of the following: admin, bible,
            facta, ficti or news.
         </li>
         <li><strong><code>author+</code>:
               </strong><br>The author of the document is either person or "unknown". 
            <ul>
               <li><strong><code>person</code>:
                     </strong><br>Person consist of following attributes: firstname, lastname,
                  born, sex and nationality.
               </li>
            </ul>
         </li>
         <li><strong><code>translator*</code>:
               </strong><br>If the document is translated, the translator is either
            "unknown" or person.
         </li>
         <li><strong><code>translated from*</code>:
               </strong><br></li>
         <li><strong><code>year?</code>:
               </strong><br>Publishing year, or if the document is unpublished, the year the
            document was written.
         </li>
         <li><strong><code>place?</code>:
               </strong><br>Publishing place, or if the document is unpublished, the country
            or place where the document was written.
         </li>
         <li><strong><code>publChannel</code>:
               </strong><br>Either publication or unpublished 
            <ul>
               <li><strong><code>publication</code>:
                     </strong><br>Consists of elements publisher, ISBN and ISSN.
               </li>
            </ul>
         </li>
         <li><strong><code>collection</code>:
               </strong><br>The name of the journal, book or article collection.
         </li>
         <li><strong><code>wordcount</code>:
               </strong><br></li>
         <li><strong><code>availability</code>:
               </strong><br>Either free or license 
            <ul>
               <li><strong><code>license</code>:
                     </strong><br>License type is "standard" or "other". The type "standard"
                  indicates a license agreed with the basic contract
               </li>
            </ul>
         </li>
         <li><strong><code>submitter?</code>:
               </strong><br>The person or instance who stored the document to the database.
            Contains elements for name and email address.
         </li>
         <li><strong><code>multilingual?</code>:
               </strong><br>If the document is multilingual, this element describes the
            languages that occur in the document. 
            <ul>
               <li><strong><code>language+</code>:
                     </strong><br>Language is given as an xml:lang attribute.
               </li>
            </ul>
         </li>
         <li><strong><code>origFileName?</code>:
               </strong><br>The original name of the file. The filename may change during
            the conversion to xml.
         </li>
         <li><strong><code>metada</code>:
               </strong><br>Either "complete" or "uncomplete".
         </li>
         <li><strong><code>version</code>:
               </strong><br>Contains version information of different conversion tools.
         </li>
      </ul>
      <h2>body</h2>
      <p>The document body contains sections and text-entities
         (<code>%text.ent</code>):
      </p>
      <ul>
         <li><strong><code>%text.ent</code>:
               </strong><br>Basic text-entites are list, table, p and pre.
         </li>
         <li><strong><code>section </code>:
               </strong><br>Contains sections and text-entities.
         </li>
         <li><strong><code>list</code>:
               </strong><br>Consists of paragraphs <code>p</code> with type "listitem".
         </li>
         <li><strong><code>table</code>:
               </strong><br>Consist of rows: 
            <ul>
               <li><strong>row:
                     </strong><br>Contains paragraphs <code>p</code> with type
                  "tablecell".
               </li>
            </ul>
         </li>
         <li><strong><code>p</code>:
               </strong><br>The paragraph type is given as an attribute: title, listitem,
            text or tablecell. Paragraph contains text and elements em, hyph and
            error: 
            <ul>
               <li><strong><code>em</code>:
                     </strong><br>Emphasis type: bold, italic, underline or delimited.
               </li>
               <li><strong><code>hyph</code>:
                     </strong><br>Empty hyphenation tag
               </li>
               <li><strong><code>error</code>:
                     </strong><br>Error marking in the text, has the correct replacement as an
                  attribute.
               </li>
            </ul>
         </li>
         <li><strong><code>pre</code>:
               </strong><br>Programlisting or other unformatted text.
         </li>
      </ul>
      <h1>Character decoding: samiChar::Decode.pm</h1>
      <p>Some of the sámi characters are wrongly utf-8-encoded by the
         conversion tools, like pdftotext. There is a Perl module
         samiChar::Decode.pm for decoding the sámi characters.
      </p>
      <h2>SYNOPSIS</h2>
      <pre xml:space="preserve">
           use samiChar::Decode;

           my $file = "file.txt";
           my $outfile = "file.txt";
           my $encoding;
           my $lang = "sme";

           $encoding = &amp;guess_encoding($file, $lang);
           &amp;decode_file($file, $encoding, $outfile);

           $encoding = &amp;guess_text_encoding($file, $outfile, $lang);
           &amp;decode_text_file($file, $encoding, $outfile);

</pre>
      <h2>DESCRIPTION</h2>
      <p>samiChar::Decode.pm decodes characters to utf-8 byte-wise, using
         code tables. It is planned for decoding the Saami characters in a
         situation, where the document is converted to utf-8 without knowing
         the original encoding. The decoding is implemented by using code table
         files, so the module can be used for other conversions as well. The
         output is however always utf-8.
      </p>
      <p>The function <code>guess_encoding </code>is used for guessing the
         original encoding. It takes into account only the most common Sámi
         characters and their frequency in the text. The file is further
         decoded using the function <code>decode_file.</code>The functions
         <code>guess_text_encoding </code>and <code>decode_text_file </code>can
         be used for guessing the encoding and decoding a file which is not
         utf-8 encoded but in it's original encoding. This is the case with
         many text files that are not converted by any tool. Thes functions are
         implemented using the iconv conversion tool.
      </p>
      <h3>Code tables</h3>
      <p>Code tables are text files with the following format: Three
         space-separated columns:
      </p>
      <pre xml:space="preserve">
       Column #1 is the input char (in hex as 0xXX or 0xXXXX))
       Column #2 is the Unicode char (in hex as 0xXXXX)
       Column #3 the Unicode name
</pre>
      <p>Most of the code tables are available at the Unicode Consortium:
         <a href="ftp://ftp.unicode.org/Public/MAPPINGS/">ftp://ftp.unicode.org/Public/MAPPINGS/</a></p>
      <p>. Some of the code tables like samimac_roman and levi_winsam are
         composed from two code tables, the one that is used as input
         encoding and another that is used as the file was converted to
         utf-8.
      </p>
      <pre xml:space="preserve">
       samimac_roman: codetables samimac.txt and ROMAN.txt
       levi_winsam: codetables levi.txt and CP1258.txt
</pre>
      <p>levi.txt and samimac.txt are available under Trond’s home page
         at: <a href="http://www.hum.uit.no/a/trond/smi-kodetabell.html">smi-kodetabell.html</a>.
         The codetables are composed using the function
         "combine_two_codings($coding1, $coding2, $outfile)" which is
         available in this package.
      </p>
      <p>These encodings are available:</p>
      <pre xml:space="preserve">
       latin6 =&gt; iso8859-10-1.txt
       plainroman =&gt; ROMAN.txt
       CP1258 =&gt; CP1258.txt
       iso_ir_197 =&gt; iso_ir_197.txt
       samimac_roman =&gt; samimac_roman.txt
       levi_winsam =&gt; levi_CP1258.txt
       8859-4 =&gt; 8859-4.txt
       winsam =&gt; winsam.txt
</pre>
      <h3>Guessing the input encoding</h3>
      <p>The original input encoding is guessed by examining the text and
         searching the most common characters. The unicode characters in hex
         are listed in hash %Char_Tables for North Saami for example. The
         uncommented characters are the ones that take part into guessing the
         encoding.
      </p>
      <p>The encodings are listed in the hash %Charfiles, they are tested
         one at the time. The occurences of the selected characters in that
         encoding are counted and the one with most occurences is returned.
         There is a place for more statistical analysis, but this simple test
         worked for me.
      </p>
      <p>If there is no certain amount of characters found, the test
         returns -1, which means that the characters should be already
         correctly utf-8 encoded. Or, the encoding was not found from the
         code tables.
      </p>
      <h2>INSTALL</h2>
      <p>To your own computer: copy the directory
         <code>gt/script/samiChar</code> to
         <code>/System/Library/Perl/5.8.6/</code>. The module is now installed
         to victorio.
      </p>
      <h1>Web upload interface</h1>
      <p>The main page of the web upload interface is <a href="http://www.divvun.no/upload/upload_corpus_file.html">http://www.divvun.no/upload/upload_corpus_file.html</a></p>
      <p>The source of the page can be found in directory
         xtdoc/sd/src/documentation/content/xdocs/upload/. There are two
         cgi-scripts involved in the upload, upload.cgi and xsl-process.cgi. They
         are both in gt/script/cgi-scripts/ -catalog. upload.cgi uploads the file
         the user has selected, converts it to xml and prints out the
         metainformation form. The xsl-process.cgi handles the metainformation
         form and stores the contents of the fields to the file-specific
         xsl-file. The xml-file is converted once more with the updated
         metainformation.
      </p>
      <p>All the files remain in the corp/tmp -directory. Every succeeded
         upload triggers an email message to the maintainer, who has to move the
         files manually to their place. The email notification is send as well if
         there is an error during the upload.
      </p>
      <p>The file names are changed to secure ones and orig-hierarchy is
         checked for a file with the same content.
      </p>
      <h1>Corpus DTD</h1>
      <p><a href="corpus_dtd.html">Corpus DTD</a></p>
   </body>
</html>