<html xmlns:xi="http://www.w3.org/2001/XInclude" lang="en">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Implementation plan for corpus interfaces</title>
      <authors>
         <person email="saara.huhmarniemi@helsinki.fi" name="Saara Huhmarniemi"></person>
      </authors>
   </head>
   <body>
      <h1>Introduction</h1>
      <fixme author="Tomi, Børre, Trond ">This document describes corpus work
         until september 2004, as found in the catalogue gt/cwb/. In 2005 we have
         made a separate corpus. This should be documented as well.
      </fixme>
      <h1>Implementation plan for corpus interfaces</h1>
      <p>The document is partly based on the discussion with the people in
         Textlaboratoriet (<a href="../admin/memos/oslo-2004-5.html">the
            memo</a>), and discussions in <a href="http://www.ling.helsinki.fi/uhlcs/saletek/jokkmokk.shtml">Saletek
            seminar, July 2004</a>.
      </p>
      <h2 id="overview">Overview and introduction</h2>
      <p>The main goal of the corpus subproject is to build an extensive and
         versatile source of text materials for Saami languages. The main goals
         of the project is to collect and develop the corpora in co-operation
         with the owners of the texts and to provide easy access to the text
         materials for non-commercial purposes such as research. The text
         materials will be available through a query processing tool: a tool
         with which a user can fetch different types of data from the Sámi
         corpora.
      </p>
      <p>The available texts will be moved to a corpus database which is
         accessed by the users through a web-interface, cf. .
      </p>
      <!--
    <img alt="" src="architecture.jpg" />
    -->
      <p><a href="architecture.jpg">The overall architecture of the
            system</a></p>
      <p>Where the material is available in different languages such as New
         Testament, a parallel corpora is created.
      </p>
      <p>This document describes the plans for implementing the corpus
         database and the query system.
      </p>
      <p>All the material concerning the corpus project is currently stored
         in the directory <code>gt/cwb/</code> under cvs.
      </p>
      <h2 id="database">The corpus database</h2>
      <p>The morphologically analyzed texts are stored in XML-format, which
         forms the corpus database. The XML-format is used as the base format
         for creating different views to the corpora. The queries are made
         available by a corpus processor tool. The project has licensed corpus
         software tools from IMS Stuttgart (<a href="http://www.ims.uni-stuttgart.de/projekte/CorpusWorkbench">the
            CQP of the IMS Stuttgart Corpus bench</a>). The IMS Cropus Workbench
         is a tool for performing searches to large text corpora.
      </p>
      <p>The CWB-toolbox is installed to victorio, the usage of the tools is
         briefly introduced in section <a href="#imsdemo">IMS Corpus Workbench:
            demo</a></p>
      <p>The XML-format of the corpus texts is documented in section <a href="#xml">XML-format of the corpus files</a>.
      </p>
      <p>In addition to actual texts, the corpus database contains other
         textual information such as author, date, genre and region that can be
         exploited for example in studies considering for example regional or
         historical variety. The other textual information is stored in
         separate header files, documented in section <a href="#meta">Meta
            information</a>.
      </p>
      <p>The work flow of converting the available text material to the
         corpus database includes the following steps:
      </p>
      <ol>
         <li>Cleaning of the original text</li>
         <li>Preprocessing</li>
         <li>Morphological tagging</li>
         <li>Manual check</li>
         <li>Conversion to XML-format</li>
         <li>Adding meta information</li>
         <li>Conversion to IMS-format</li>
      </ol>
      <p>The texts often arrive in some other than a plain text format, such
         as pdf or MS Word. There are some tools for cleaning both types of
         texts, as well as html. The tools and usage are described in section
         <a href="#cleaning">Cleaning up the original text</a>. This step also
         involves the character conversion to the project internal 7-bit
         encoding (the special characters are presented as digraphs: c1, s1,
         etc.).
      </p>
      <p>The next step is to cut the text in the sentences and word tokens.
         The preprocessor is documented in <a href="preprocessor.html">preprocessor.html</a>. The preprocessor tool
         may have to be adapted to the corpus project, if the text contains for
         example some xml-formatting. The modifications are not yet implemented
         nor planned.
      </p>
      <p>Step three is implemented by analysis and disambiguation tools.</p>
      <p>As long as there are problems with either preprocessing or analysis
         and disambiguation the step three, manual check, is hard work. When
         the other tools are reliable only spot checks are needed.
      </p>
      <p>The meta information is described in section <a href="#meta">Meta
            information</a>.
      </p>
      <p>The conversion to XML-format is described in <a href="#xml">XML-format of the corpus files</a></p>
      <p>The conversion to IMS-format is not yet implemented, nor fully
         planned. In this part, we rely to the help of <a href="http://www.hf.uio.no/tekstlab/">Textlaboratoriet</a> in the
         University of Oslo.
      </p>
      <h3 id="cleaning">Cleaning up the original text</h3>
      <p>The cleaning of the corpus documents involves removing all the
         formatting which is not relevant for the corpus database or
         morphpological analysis. The original shape of the document should
         be preserved: the headings and lists as separate paragraphs etc.
         Perhaps later we want to move some formatting to XML as well.
      </p>
      <p>There are couple of tools installed for cleaning the texts: <a href="http://www.winfield.demon.nl/">antiword</a> and <a href="http://wvware.sourceforge.net/">wvWare</a>. Antiword does
         simple word to text and html converting, wvWare involves more
         formats and conversion options.
      </p>
      <p>The documentation of antiword is <a href="/tools/antiword.man">antiword.man</a> and the usage for
         example converting an utf-8 coded MS Word document to the 7bit
         project-internal format:
      </p>
      <pre xml:space="preserve">$ antiword -m UTF-8.txt file.doc | utf8-7bit.pl &gt; file.txt
</pre>
      <p>The information of wvWare can be found from packages' man
         page:
      </p>
      <pre xml:space="preserve">$ man wvWare
</pre>
      <h3 id="xml">XML-format of the corpus files</h3>
      <p>The XML format of the analyzed text is basically the
         following:
      </p>
      <pre xml:space="preserve">&lt;text&gt;
 &lt;sentence&gt;
  &lt;token form="The" lemma="the" POS="DET" /&gt;
  &lt;token form="flies" lemma="fly" POS="N" /&gt;
 &lt;/sentence&gt;
&lt;/text&gt;
</pre>
      <p>Optionally, one can</p>
      <ul>
         <li>enclose several texts in a corpus-tag</li>
         <li>use a paragraph-tag enclosing several sentence-s</li>
         <li>use more attibutes:<br> 
            <ul>
               <li>on all elements: type and language</li>
               <li>on tokens: morphology, syntax etc.</li>
            </ul>
         </li>
         <li>have several readings for each token: 
            <pre xml:space="preserve">       &lt;token form="flies"&gt;
       &lt;reading lemma="fly" POS="N" /&gt;
       &lt;reading lemma="fly" POS="V" /&gt;
       &lt;/token&gt;
      </pre>
            </li>
      </ul>
      <p>There is a first version of the dtd corpus.dtd for the format. In
         addition, there is a file sme_tagset.ent which contains the names of
         the tag classes. This is supposed to make the dtd more flexible,
         since the tag classes may change among languages.
      </p>
      <p>The conversion from CG2-output to XML is handled by a script
         convert2xml, the script requires the tag file korpustags.txt to get
         the tagsets right.
      </p>
      <p>In the applications, the Perl modules such as XMLTwig are used
         for parsing XML. Emacs is a fairly good tool for editing XML, but it
         might be a good idea to install a separate xml-processor as well.
         Apache's <a href="http://xml.apache.org/">Xerces</a> seems to be a
         good and widely used tool for xml parsing and generation.
      </p>
      <h3 id="meta">Meta information</h3>
      <p>The structural information is encoded in XML-format, following
         for example the CES standard. Then there would be three different
         categories of information for each corpus: Global information of the
         text and its content: author, character set, etc. corresponding TEI
         header. Primary data, which includes structural units of text etc.
         abbreviations and so on. And linguistic annotation, including
         morphological and syntactic information, alignment etc. The queries
         to the documents would then be made by tools designed for processing
         XML.
      </p>
      <p>However, the query system offered by IMS Corpus Workbench does
         not support SGML in full extent. Rather, the structural information
         offered by IMS-tools is rather restricted. The query engine CQP uses
         regular expressions in corpus queries, which is a desired feature.
         The structural information cannot be queried at all by CQP, it is
         only available in the results.
      </p>
      <p>The global information can be transferred to CQP searchable
         format, for example by transferring the header information to
         attributes in IMS. The header information may also be consisted as a
         string in one attribute.
      </p>
      <p>The exact format of the corpus header files is not yet
         planned.
      </p>
      <h3 id="ims">Transforming the corpus files to IMS-format</h3>
      <p>The "Corpus administrator's Manual" describes in detail, how the
         text corpus is transformed to the internal representation used by
         the IMS toolbox. As we have desided to use XML for the basic format
         of the corpora, suitable conversion tools from XML to the format
         required by IMS have to be developed.
      </p>
      <p>There will be conversion scripts from XML-format to TEI and IMS
         corpus workbench, provided by Textlaboratory.
      </p>
      <h3 id="web">The web-interface</h3>
      <p>The web-interface will be provided by Textlaboratory.</p>
      <h2 id="files">Files and directories</h2>
      <p>The corpus files themselves will be placed to
         <code>/usr/local/share/corp/</code> for now. The subdirectory
         <code>doc</code> contains the original texts in their original
         formatting. Later, there should be separate directory for all the
         corpus files.
      </p>
      <p>The location of the corpora has to be planned with Roy, the files
         can be quite big and need not to be backuped daily (but weekly or
         monthly will be ok). Perhaps some globally accessible, separate
         filesystem, for example directory <code>/corpora</code>.
      </p>
      <p>At the moment the corpus files are stored to cvs. The corpus files
         are modified all the time for testing purposes, so cvs is ok. Also the
         size of the corpus is fairly small, about 34M altogether. Tagged
         corpus is obviously much bigger but at the development phase it will
         not cause any problems.
      </p>
      <p>However, the usage of cvs for storing large corpora is impossible
         if the files gets much bigger. This is because every user has his own
         copy of all the files and also the modifications between versions that
         are stored to repository may grow. The size of ims-format corpus can
         be some 10-50 times bigger than the original raw text, depending on
         the amount of tags (the number is just a hasty estimate).
      </p>
      <h3 id="cwb">The CWB installation and directories</h3>
      <p>The version of the software is 3.0 and the installed archive name
         was <code>cwb-2.2.b72-i386-linux.tar.gz</code>. The up-to-date
         information was available at <a href="ftp://ftp.ims.uni-stuttgart.de/pub/outgoing/cwb-beta/index.html">ftp://ftp.ims.uni-stuttgart.de/pub/outgoing/cwb-beta/index.html</a>.
      </p>
      <p>The software is installed to directory
         <code>/usr/local/cwb</code>. The environment variable PATH has to be
         updated:
      </p>
      <pre xml:space="preserve">export PATH=$PATH:/usr/local/cwb/bin
</pre>
      <p>There are specific corpus registry files which contain
         information on the corpus, like where the data is stored. The
         registry files should be in one place, perhaps in the same place as
         the corporal, in directory <code>/corpora/registry</code>. The
         environment variable <code>CORPUS_REGISTRY</code> has to be set.
      </p>
      <h2 id="info">Information in the corpus</h2>
      <h3 id="morphinfo">Morphological information in the
         corpus
      </h3>
      <p>The corpus contains tokens (words) and other positional
         attributes such as part-of-speech tags. The tags are arranged one in
         each column. The columns are separated by tab.
      </p>
      <h4>Corpus tags</h4>
      <p>There are the following tag categories:</p>
      <ul>
         <li>Part of speech: N A V Adv Pron CS CC Adp Po Pr Interj Pcle
            Num
         </li>
         <li>Number or Person/Number: Sg Pl Sg1 Sg2 Sg3 Du1 Du2 Du3 Pl1 Pl2
            Pl3 ConNeg
         </li>
         <li>Case: Ess Nom Gen Acc Ill Loc Com</li>
         <li>Possessive suffix: PxSg1 PxSg2 PxSg3 PxDu1 PxDu2 PxDu3 PxPl1
            PxPl2 PxPl3
         </li>
         <li>Clitic: Qst Foc</li>
         <li>Grade: Comp Superl</li>
         <li>Attribute: Attr</li>
         <li>Derivation (We must consider whether to use these..): Pass h
            upmi ...
         </li>
         <li>Polarity: Neg</li>
         <li>Mood: Ind Pot Cond Imprt ImprtII</li>
         <li>Tense: Prs Prt</li>
         <li>Nominal verb form: Inf Act PrsPrc PrfPrc VGen VAbess Ger
            ConNeg ConNegII
         </li>
         <li>Pronoun type: Pers Dem Interr Refl Recipr Rel Indef</li>
         <li>Other: PUNCT CLB ABBR ACR</li>
         <li>Punctuation type: LEFT RIGHT</li>
         <li>Num type: Ord Card</li>
      </ul>
      <h3 id="structinfo">Structural information</h3>
      <p>It is possible to mark for example the beginning and end of a
         sentence to the corpus file by using SGML-like markers. Whether we
         should use that or not is dependent upon what benefits it may give
         us, seen from the ims framework point of view. Changing the tag CLB
         etc. to SGML-like markers is not a problem, but it is unclear to
         what extent it helps either parsing or corpus processing.
      </p>
      <p>Large units of discourse information are:</p>
      <ul>
         <li>headings</li>
         <li>lists</li>
         <li>chapters</li>
         <li>paragraphs?</li>
         <li>footnotes etc.</li>
      </ul>
      <p>Smaller units:</p>
      <ul>
         <li>sentences</li>
         <li>abbreviations</li>
         <li>dates</li>
         <li>quotations</li>
         <li>names</li>
      </ul>
      <p>We have to find out what kind of information it is possible to
         extract from diffeent types of documents, and how much of the
         structural information can be extracted automatically.
      </p>
      <p>In Microsoft Word format, the information is in the underlying
         representation. A priori, it should be possible to write an MSW
         macro to turn this into textual informaion prior to the "save as
         enriched text" command that we use to convert MSW documents to our
         internal format. Seen from a disambiguation point of view,
         information on paragraphs and bulletpoint lists is clearly a
         valuable resource, if we can write rules that rely on such
         information (demand finite verbs form sentences, not from titles,
         parenthesis fragments or bulletpoint items).
      </p>
      <h2 id="imsdemo">IMS Corpus Workbench demo</h2>
      <p>IMS Corpus Workbench is now installed to victorio and can be tested
         with two demo corpuses. There is English demo corpus which consists of
         Charles Dickens novels and German demo corpus of law texts. The
         corpora is accessed using the corpus query processor CQP. To get CQP
         working, add these lines to your <code>.bashrc</code>:
      </p>
      <pre xml:space="preserve">    export PATH=$PATH:/usr/local/cwb/bin
    export CORPUS_REGISTRY=/usr/local/cwb/registry
</pre>
      <p>Start CQP by typing</p>
      <pre xml:space="preserve">    cqp
</pre>
      <p>To the shell prompt. Leave the program by typing <code>exit;</code>
         or <code>Ctrl-D</code>. I recommend to turn off the highlighting
         by
      </p>
      <pre xml:space="preserve">    set Highlighting no;
</pre>
      <p>The command <code>show;</code> shows the installed corpora. To
         select the Dickens corpus, type
      </p>
      <pre xml:space="preserve">    DICKENS;
</pre>
      <p>To make a query, follow the instructions in the CQP Tutorial (path:
         <code>/usr/local/cwb/doc/CQP-Tutorial.2up.pdf</code>).
      </p>
      <h3>Saami demo corpus</h3>
      <p>There is one short Saami demo corpus with limited tags, stme1029.
         To make queries to it, type
      </p>
      <pre xml:space="preserve">    STME1029;
</pre>
      <p>to cqp prompt.</p>
      <p>The tags used in the corpus are listed in the tag list:
         <code>gt/cwb/korpustags.txt</code>. Commented lines are marked with
         '%', the line that starts with hash (#), marks the tag class, e.g.
         POS. Under it is the list of the tags which belong to that class, in
         case of POS, N, Adj, V, etc.
      </p>
      <p>The corpus file is first converted to a format where each word is
         in its own line followed by base form and the tags associated to it.
         Tags are separated by TAB. See file
         /usr/local/cwb/demo/stme1029/stme1029.vrt (in directory
         <code>/usr/local/cwb/demo/stme1029</code> for example. The coversion
         from CG2 output to word-list format is done automatically by using
         script <code>convert2cwb</code>.
      </p>
      <h2 id="other">Some not so relevant documentation</h2>
      <h3>XCES</h3>
      <p>XCES is an XML-version of CES. It allows the usage of XML-tools
         to corpus files. The tag names are the same as in CES. In practise,
         a corpus file is divided two sections, header and body text. The
         header is encoded using XCES, the text section with morphological
         and syntactic tags in IMS tabular format. We will see, how to ensure
         the compatibility of these two formats. Either we could have the
         XCES header in a separate file. INL's (Institutt for nordistikk og
         litteraturvitskap) corpus project SLM (Seksjon for lexikografi og
         mï¿½fregransking) has solved the problem som way, so I will trust
         that it is possible for us too. Next I will go through INL's header
         specifications and see how they should be modified for the Sami
         corpora.
      </p>
      <h4>Tag specifications</h4>
      <p>INL have different kind of headers for different type of texts.
         We have to see if that is necessary. We have different types of
         corpora, books, articles, law texts, etc.
      </p>
      <p>bokerSA.txt <strong>cesDoc</strong> starts the document. It
         contains document id <strong>id</strong> and language
         <strong>lang</strong>.
      </p>
      <p><strong>cesHeader</strong> starts the header. It has the
         following sections:
      </p>
      <pre xml:space="preserve">&lt; fileDesc&gt; &lt; /fileDesc&gt;
&lt; encodingDesc&gt; &lt; /encodingDesc&gt;
&lt; profileDesc&gt; &lt; /profileDesc&gt;
&lt; revisionDesc&gt; &lt; /revisionDesc&gt;
</pre>
      <p><strong>fileDesc</strong> for the bibliographic description of
         the corpus.
      </p>
      <p>I describe only the content elements.</p>
      <ul>
         <li><strong>h.title</strong> contains the name of the
            document.
         </li>
         <li><strong>respStmt</strong> is the information about the
            person or institute of the intellectual content of the
            text.
         </li>
         <li><strong>wordCount</strong> is the count of words in the
            text,
         </li>
         <li><strong>byteCount</strong> the number of bytes (the text
            together with its markup).
         </li>
         <li><strong>extNote</strong> is for additional information
         </li>
         <li><strong>distributor</strong> gives the name of institution
            who distributes the text or corpus.
         </li>
         <li><strong>pubAddress</strong> postal address of the
            distributor
         </li>
         <li><strong>eAddress</strong> the distributor's electronic
            address
         </li>
         <li><strong>availability</strong> the restrictions to the use or
            distribution, e.g. copyright
         </li>
         <li><strong>pubDate</strong> the publication date
         </li>
         <li>If the text is published not as an independent publication,
            the following information is inside
            <strong>analytic</strong>-tag. Otherwise the tag
            <strong>monogr</strong> is used. 
            <ul>
               <li><strong>h.title</strong> full title
               </li>
               <li><strong>h.author</strong> name of the author
               </li>
               <li><strong>edition</strong> edition
               </li>
            </ul>
         </li>
         <li><strong>publisher</strong></li>
         <li><strong>pubDate</strong></li>
         <li><strong>pubPlace</strong></li>
         <li><strong>idno</strong> ISBN or equivalent
         </li>
         <li><strong>biblScope</strong> The scope of the bibliographic
            reference, e.g. page numbers
         </li>
         <li><strong>biblNote</strong> note
         </li>
      </ul>
      <p>The encoding description <strong>encodingDesc</strong>
         describes the relation between the text and its original
         source.
      </p>
      <ul>
         <li><strong>taxonomy</strong> The classification taxonomy
         </li>
         <li><strong>category</strong> The category in the taxonomy
         </li>
      </ul>
      <p>The Profile description <strong>profileDesc</strong>, contains
         the language etc.
      </p>
      <ul>
         <li><strong>language</strong> The id of the language. If more
            than one language is specified, it is possible to use &lt;
            foreign xml:lang=nb&gt; bokmï¿½ word &lt;/foreign&gt; tags in the
            text.
         </li>
         <li><strong>writingSystem</strong> specifies the writing system,
            there can be more than one specified as well.
         </li>
         <li><strong>textClass</strong> text classification scheme and
            keywords.
         </li>
         <li><strong>catRef</strong> defines the classification taxonomy
            with reference to <strong>taxonomy</strong> and the category in
            the taxonomy, with reference to <strong>category</strong>.
         </li>
         <li><strong>translation</strong> information about possible
            translations of the text.
         </li>
         <li><strong>trans.loc</strong> location of the
            translations.
         </li>
         <li><strong>translator</strong> name of the translator
         </li>
      </ul>
   </body>
</html>