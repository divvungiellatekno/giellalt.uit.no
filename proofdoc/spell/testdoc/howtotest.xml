<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V2.0//EN"
"http://forrest.apache.org/dtd/document-v20.dtd">
<document xml:lang="en">
  <header>
    <title>How to test the spellers</title>
  </header>

  <body>
    <section>
      <title>Automatic testing</title>

      <p>We now have a test bench for automatic testing of the spellers, using
      different data sets. The data sets/tests serve different purposes, and
      are the following:</p>

      <dl>
        <dt>regression-test</dt>

        <dd>tests the speller against a set of known problematic misspellings
        and correct words, to check that newer versions don't break earlier
        fixes; the data set will often contain "constructed" words made to
        highlight certain morphological constructions</dd>

        <dt>typos-test</dt>

        <dd>tests the speller against a collection of real typographic errors
        and their corrections, as found in our corpus documents or elsewhere
        in real texts â€” the purpose of the test is to see how well the speller
        handles real errors, both when it comes to detecting them, and to give
        the correct suggesion; the data set should <em>never</em> contain
        constructed errors</dd>

        <dt>baseform-test</dt>

        <dd>extracts all baseforms (=nominative singular, infinitive) found in
        our lexicons, and send them through the speller; to ensure that the
        lexicon is well-formed and that the speller actually recognises all
        (baseforms of the) words it should</dd>

        <dt>correct-test</dt>

        <dd>runs complete documents manually marked up with error/correction
        tags through the speller; this test will check both lexical coverage,
        calculate precision, recall and coverage, as well as give an idea of
        the quality of the suggestions; for details on marking up documents to
        be used as input, see <a href="error-markup.html">this page</a></dd>
      </dl>

      <p>Below we have briefly described how to run these automatic tests, how
      to read the test reports, and then some more details on each test.</p>

      <section>
        <title>Running automatic tests, storing results</title>

        <p>To run each of the automatic tests above, just <code>make</code>
        the test name as given, and the <code>TARGET</code> as usual (in the
        <code>gt/</code> directory), e.g.:</p>

        <source>make regression-test TARGET=sme</source>

        <p>There is one exception, and that is the correct-test, which also
        requires a DOC input parameter - the correct document used as input
        data:</p>

        <source>make correct-test TARGET=sme DOC=somedoc.correct.doc.xml</source>

        <p>There is a short-cut <code>make</code> target that will run all but
        the <code>correct-test</code> at once:</p>

        <source>make spelltest TARGET=smj #will run regression, typos &amp; baseform</source>

        <p>In addition, it is possible to specify the tool used for the actual
        testing, that is, the speller engine, by giving <code>make</code> the
        parameter <code>TESTTOOL</code>, with one of the following values:</p>

        <dl>
          <dt>pl</dt>

          <dd>polderland command-line speller</dd>

          <dt>mw</dt>

          <dd>Microsoft Word as the engine, iterating over each of the words
          in the input data, and asking Word about its spelling status;
          AppleScript is used to tell Word what to do, and to collect the
          response from Word</dd>
        </dl>

        <p>In the future, more spelling engines will be added, like hunspell
        (<strong>hu</strong>) and possibly aspell (<strong>as</strong>).</p>

        <p>The <strong>mw</strong> test engine has some shortcomings due to
        Word's AppleScript implementation (or our inability to find our way
        through the Word AppleScript dictionary), but it also has the nice
        feature to be comletely independent of the real speller engine behind
        Word. This means that it is possible to test other spellers than ours,
        and compare the test results across languages and speller engines
        (given reasonably similar input data).</p>

        <p>It is also possible to add the date of the test run as a parameter
        to <code>make</code>, if one for example would like to update an
        earlier test run with corrected test data. This is done with the
        parameter <code>DATE</code>. A full <code>make</code> command for the
        future hunspell tool would then look something like:</p>

        <source>make correct-test TARGET=sme DOC=somedoc.correct.doc.xml TESTTOOL=hu DATE=20071020</source>

        <p>The output from each test is two xml files, both stored in
        <code>gt/doc/proof/spelling/testing/</code>. One is a bare-bones
        standardised xml representation of the speller output, the other is a
        Forrest-doc xml file presenting both the direct test results and some
        calculated statistics. To save the test results for the future and at
        the same time make them available for others, the xml files should be
        checked in in <code>cvs</code>.</p>

        <p>Finally, to properly include the test results in the Forrest-driven
        site of ours, the forrest-doc files should also be added to the menu
        system by including a reference in the file
        <code>gt/doc/site-proof-frag.xml</code>.</p>
      </section>

      <section>
        <title>regression-test</title>

        <section>
          <title>Input data</title>

          <p>The <code>regression-test</code> input data is stored i the file
          <code>$TARGET/polderland/regressions.txt</code>. The format is quite
          simple, and has two forms:</p>

          <source>error&lt;TAB&gt;correction&lt;TAB&gt;#comment
correct&lt;TAB&gt;&lt;TAB&gt;!comment</source>

          <p>Comments can either start with # or !. The first variant is a
          so-called <em>negative</em> test, where the speller should detect
          the <code>error</code> and give the <code>correction</code> as one
          of its suggestions. The other variant is consequently a
          <em>positive</em> test, where we check that the speller actually
          recognises correct word forms. Often missing correct suggestions or
          false negatives are caused by the <em>correct</em> form not being
          recognised. The positive tests will help in detecting such
          cases.</p>
        </section>

        <section>
          <title>Reading the test report</title>

          <p>The test report for the regression tests have seven main
          sections:</p>

          <ol>
            <li>Overview</li>

            <li>True positives</li>

            <li>False positives</li>

            <li>False negatives</li>

            <li>True negatives</li>

            <li>Grouped by bug #</li>

            <li>Testpairs not in bugs</li>
          </ol>

          <p>Each section is briefly described below.</p>

          <section>
            <title>Overview</title>

            <p>This gives some basic statistics about the regression test. The
            most important figures here are the false negatives and false
            positives - they indicate how many testpairs are still
            failing.</p>
          </section>

          <section>
            <title>True positives</title>

            <p>Normally not very relevant reading - these are the correctly
            recognised misspellings.</p>
          </section>

          <section>
            <title>False positives</title>

            <p>This section lists correct input flagged as misspellings. Check
            this briefly to see if there are any patterns in the incorrectly
            flagged words. Often a few bugs are failing, so further
            investigation should be directed there.</p>
          </section>

          <section>
            <title>False negatives</title>

            <p>This is misspellings not detected by the speller. Again, check
            whether there is a pattern among the undetected misspellings.</p>
          </section>

          <section>
            <title>True negatives</title>

            <p>Normally not very relevant reading - these are the correctly
            recognised correct words.</p>
          </section>

          <section>
            <title>Grouped by bug #</title>

            <p>This is really the most relevant section. Here, all failings
            have a <em class="broken">light red background</em>, to make them
            stand out visually and be easy to spot. To get an overview of the
            situation for reported bugs, go directly to this section, and
            scroll through it looking for red rows.</p>

            <p>All bugs with no red rows can be closed (or should be already),
            whereas bugs with red rows (ie broken tests) need further
            investigation.</p>

            <p>For a test pair to show up in this section, the comment column
            in the test data has to start with the bug ID.</p>
          </section>

          <section>
            <title>Testpairs not in bugs</title>

            <p>This last section contains all test pairs not covered by the
            previous section, and is using the same redish background colour
            to indicate failed tests. It should be as small as possible, as we
            want most or all test pairs to be associated with a bug.</p>
          </section>
        </section>
      </section>

      <section>
        <title>typos-test</title>

        <section>
          <title>Input data</title>

          <p>The <code>typos-test</code> input data is stored i the file
          <code>$TARGET/src/typos.txt</code>. The format is similar to the
          regression data file:</p>

          <source>error&lt;TAB&gt;correction&lt;TAB&gt;#comment</source>

          <p>Comments can either start with # or !.</p>

          <p>The data is a collection of true misspellings found in different
          sources. It should <em>NOT</em> contain any made-up examples (they
          can be put in the regression.txt file if relevant, otherwise don't
          use such data).</p>

          <p>As part of the testing, all the correct words are also extracted
          and used as input to the speller. These should all be accepted, and
          serve as positive test cases for the typos-test.</p>
        </section>

        <section>
          <title>Reading the test report</title>

          <p>The test report for <code>typos-test</code> contains the same
          first five sections as the regression-test report. The most
          important things to look at are the following points:</p>

          <dl>
            <dt>true positives without (correct) suggestions</dt>

            <dd>why are the suggestion(s) missing?</dd>

            <dt>false negatives</dt>

            <dd>any pattern in the undetected misspellings</dd>

            <dt>false positives</dt>

            <dd>any pattern in the wrongly flagged words</dd>

            <dt>overall statistics</dt>

            <dd>our target is to detect and correct as many of the known typos
            as possible</dd>
          </dl>
        </section>
      </section>

      <section>
        <title>baseform-test</title>

        <section>
          <title>Input data</title>

          <p>The <code>baseform-test</code> input data is generated as an
          extraction of all lexical entries in our <strong>LexC</strong>
          files, and is used to ensure that we actually recognise all the
          words that we put into the speller. Further, since we're really not
          interested in seeing the long list of <em>recognised</em> baseforms,
          the data is sent two times through the speller. The first round is
          used to identify all negative hits (ie all rejected baseforms), and
          the second round is used to only analyse those, to get both some
          statistics, suggestions (the suggestions can be quite telling about
          why a certain word was rejected) and filter out some cases that are
          actually recognised (the first filtering is a little
          over-active).</p>
        </section>

        <section>
          <title>Reading the test report</title>

          <p>The test report for <code>baseform-test</code> contains the same
          first five sections as the regression-test report. The most
          important things to look at are the following points:</p>

          <dl>
            <dt>number of false negatives</dt>

            <dd>this should really go down to zero</dd>

            <dt>false negative patterns</dt>

            <dd>use any patterns to try to identify why groups of baseforms
            are rejected.</dd>

            <dt>single entries</dt>

            <dd>a substantial part of the unrecognised baseforms will be
            undetected errors in the lexicon; they should just be
            corrected</dd>
          </dl>
        </section>
      </section>

      <section>
        <title>correct-test</title>

        <section>
          <title>Input data</title>

          <p>The <code>correct-test</code> input data is an xml document with
          errors and corrections marked up. The xml document is a conversion
          from a similarly marked-up corpus document, and represent our
          real-world test scenario for our spellers (the other test cases are
          different types of more technical testing).</p>

          <p>This test will usually have to be run several times on a new test
          document, as the first run will reveal inconsistencies and mistakes
          in the error/correction markup that needs to be fixed before we get
          reliable test results.</p>

          <p>The test document is read by <code>ccat</code>, which produces
          test data in a format identical to the other test types, that
          is:</p>

          <source>error&lt;TAB&gt;correction&lt;TAB&gt;
correct&lt;TAB&gt;&lt;TAB&gt;</source>

          <p>Since the input data is a complete document, it is possible to
          calculate reliable statistics on precision and recall.</p>
        </section>

        <section>
          <title>Reading the test report</title>

          <p>The test report for <code>baseform-test</code> contains the same
          first five sections as the regression-test report. The most
          important things to look at are the following points:</p>

          <dl>
            <dt>test statistics</dt>

            <dd>in the <code>correct-test</code>, the precision and recall
            figures are real measures of the quality of our speller, and
            should be thoroughly followed between speller versions.</dd>

            <dt>false negatives</dt>

            <dd>that is, undetected spelling errors - these should be as few
            as possible</dd>

            <dt>false positives</dt>

            <dd>this number should also be low, although it is normally not
            possible to get down to zero</dd>

            <dt>true positives without (correct) suggestions</dt>

            <dd>we want to be able to correct as many of the detected
            misspellings as possible, which makes this category an interesting
            study object; it should be as small as possible</dd>
          </dl>
        </section>
      </section>
    </section>

    <section>
      <title>Manual testing</title>

      <section>
        <title>Program Settings</title>

        <p>In order to obtain measurable results, we set up the programs in
        the same way:</p>

        <dl>
          <dt>Common settings</dt>

          <dd><ul>
              <li>Check Upper case words (turn off "Ignore Upper case")</li>

              <li>Check words with numbers (turn off "Ignore words with
              numbers")</li>

              <li>Ignore words with numbers (leave this options on)</li>
            </ul></dd>

          <dt>MS Off/Mac</dt>

          <dd>Word&gt;Preferences&gt;Spelling and Grammar</dd>

          <dt>MS Off/Win</dt>

          <dd>In the same location?</dd>
        </dl>
      </section>

      <section>
        <title>Types of testing</title>

        <ul>
          <li>Technical testing</li>

          <li>Linguistic testing <ul>
              <li>Testing the proofing</li>

              <li>Testing the suggestions</li>
            </ul></li>
        </ul>
      </section>

      <section>
        <title>Technical testing</title>
      </section>

      <section>
        <title>Linguistic testing: Testing the proofing</title>

        <p>We test <code>precision</code>, <code>recall</code> and
        <code>accuracy</code>. Precision measures the actions of the program:
        Given that it indicates an error, can we trust that it actually is an
        error? The recall measures the robustness of the program: Given that
        we have written a misspelled word, what are the chances that the
        program finds it? These two measures are interlinked: A strict program
        will flag for errors often, find many, but also too many. On the
        contrary, a program acting on the safe side will flag an error only
        when sure to have found one, at the expence of letting through some
        erros. The former is better When users really want a correct text, and
        the latter is better when the user is annoyed by false alarms, and
        really just wants to get rid of the worst errors, at a minimal cost.
        Accuracy measures the overall perforance, and takes both the other
        measures into account</p>

        <p>To obtain these measures we need the following data:</p>

        <dl>
          <dt>words (wds)</dt>

          <dd>The number of words in the text</dd>

          <dt>true positives (tp)</dt>

          <dd>The number of true errors found by the spellers (red
          errors)</dd>

          <dt>false positives (fp)</dt>

          <dd>The number of correctly written words claimed to be errors by
          the program (correct words in red)</dd>

          <dt>true negatives (tn)</dt>

          <dd>The number of correctly written words recognised as such
          (correct word, no red line)</dd>

          <dt>false negatives (fn)</dt>

          <dd>The numbers of errors not found by the speller (misspelling
          without redline)</dd>
        </dl>

        <p>We cound wds, tp, fp, fn, and calculate tn as wds - (tp + fp + fn).
        The test values are calculated as follows (there is a spreadsheet
        available to do this automatically):</p>

        <ul>
          <li>precision = tp/(tp+fp)</li>

          <li>recall= tp/(tp+fn)</li>

          <li>accuracy = tp+tn/all</li>
        </ul>
      </section>

      <section>
        <title>Linguistic testing: Testing the suggestions</title>

        <p>Also here, we test for <code>precision</code>, <code>recall</code>
        and <code>accuracy</code>.</p>

        <p>To obtain these measures we need the following data:</p>

        <dl>
          <dt>errors (err)</dt>

          <dd>The number of errors in the text</dd>

          <dt>true positives (tp)</dt>

          <dd>The number of true suggestions</dd>

          <dt>false positives (fp)</dt>

          <dd></dd>

          <dt>true negatives (tn)</dt>

          <dd></dd>

          <dt>false negatives (fn)</dt>

          <dd></dd>
        </dl>

        <ul>
          <li>precision = tp/(tp+fp)</li>

          <li>recall= tp/(tp+fn)</li>

          <li>accuracy = tp+tn/all</li>
        </ul>
      </section>
    </section>

    <p class="last_modified">Last modified: $Date: 2008-11-05 19:52:54 +0200 (ons, 05 nov 2008) $, by
    $Author: boerre $</p>
  </body>
</document>