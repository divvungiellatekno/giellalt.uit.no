!!!Edmonton presentation

Sjur Moshagen

!!!Overview of the presentation

* philosophy - why?
* General presentation
** organisation
** what is where
** interaction between components to produce different types of fst's
* fst compilers supported (Xerox, Hfst, Foma)

!!Dealing with descriptive vs normative grammars

* the normative is a subset of the descriptive
* tag the non-normative superset: +Err/Orth etc.
(say something about the usefulness of standardised tagsets

!!Going from analyser to speller

* coverage and overgeneration - how to restrict and enhance (filters, lexicalisations)
* tuning the suggestions - an art
* who decides on the norm and the orthography?
* different speller needs for different persons and groups

!!!Introduction

!!Philoshophy

# askldf
# Recognition - know the basic setup of one language -- know the setup of them all

!!General principles

# Be explicit (use ''non-cryptic'' catalogue and file names)
# Be clear (files should be found in non-surprising locations)
# Be consistent (keep conventions identical from language to language whenever possible)
# Be modular - both source code and build system
# Divide language-dependent and language-independent code
# Reuse resources
# Possibility for all tools to be built for all languages
# Parametrise the build process


!!!Introduction

From Antti:

Our CWIL workshop is fast approaching, and I believe it would be good to have a general presentation on the Giellatekno/Divvun architecture - how the whole system is organized, what you will find where, and how the various pieces interact in producing the various applications (not just the core FSTs, but also the different variants for electronic dictionaries, spell-checkers, and CALL applications). You could also make some note of the various FST compilers we are making use of.

Basically, I'm thinking of something quite along the lines you presented last June - but now we have many new faces, that would really benefit from the big picture, in particular the participants to the first Athabascan segment, and also the subsequent Algonquian stretch (our two new graduate students and postdoc). This would in fact involve presenting the same content twice, most likely on Sunday 7th and Friday 12th.

In addition, we could consider having an introductory presentation on the general challenges of transforming a decent descriptive FST into something that works as a (useful) spell-checker - and the various strategies of how these challenges may be addressed in tightening up the linguistic description. Myself, I could well present something based on the ambiguous analyses resulting from applying the current analyzer real Cree texts, which concretely exhibits what analyses an initial decent descriptive morphological analyzer is capable of, but many of which analyses (and forms) one would not want to allow a spell-checker to accept. But might you have something prepared for some other earlier event on this fst2speller aspect, e.g. from the SÃ¡mi perspective?
