!!!Setting the TCA2 parameters

This document discusses ways of improving the sentence alignment results provided by TCA2.


!!!Installing the last version of TCA2

Our version is from 2006. Hopefully things have happened since then. We contact Bergen and investigate.

* __Responsible__: Trond
* __Deadline__: 7.4. (no answer so far)

!!Updating the documentation to the "for the rest of us" level

All involved parties need to have a look-and-feel of the alignment in order to see what is going on.

* __Responsible__: 
* __Deadline__: 


!!!Improving the anchor list

We might parametrise the anchor list into one general part, and one thematic 
part, e.g. along the division in the corpus catalogue structure.

The existing anchor list should be both trimmed and extended.

!!Remove unused anchors

# Make an fst or corresponding of the sme (sma, smj, nob, â€¦) columns of the anchor.txt file.
# Run the fst against the parallel corpus.
# Remove unused lines from the anchor list

* __Responsible__: 
* __Deadline__:

!!Add onchors not in use

# Make a frequency list of the word forms of the sme (etc.) parallel corpus
# Run the frequency list against the anchor.fst
# Starting (almost?) from the top, and add anchors lacking from the anchor list.

* __Responsible__: 
* __Deadline__:

!!Open questions:

Is there an optimal length for the anchor list?

!!!Setting the word length parameter

The average word-length of each language involved in alignment (sme, smj, sma, nob, nno, eng) is as follows:

 sme = 9,17864; sma = 9,39066; smj = 8,91339; nob = 6,33559; nno = 7,15580; eng = 6,08796

The command used was:

{{{
  ccat -l sme -r converted/sme/admin/ | \
  preprocess --abbr=~/gtsvn/gt/sme/bin/abbr.txt | \
  grep '[a-z]'|head -100000|wc
}}}

!! List of the wordlength ratio for each language pair:

|| Lg pair || Ratio || Inverse
| sme : nob | 1,449 | 0,690
| sme : nno | 1,836 | 0,545
| sme : smj | 1,030 | 0,971
| sme : sma | 0,977 | 1,023
| sme : eng | 1,508 | 0,663
| smj : nob | 1,407 | 0,711
| smj : nno | 1,246 | 0,803
| sma : nob | 1,482 | 0,675
| sma : nno | 1,312 | 0,762

This ratio should be given as a parameter when conducting the TCA2 alignment.

!!!Weighting

The weight of TCA2 is preset at the following values ("for no scientific reason") to:

* anchor words       - 1.0
* anchor phrases     - 1.6
* proper names       - 1.3
* dice words         - 1.3
* dice phrases       - 1.6
* numbers            - 1.3
* scoring characters - 1.3

Investigate whether these values are sensible.

* __Responsible__: 
* __Deadline__:

!!Dice similarity

Quoting the documentation:

''If the first n (n is read as a parameter to the program) characters are equal for a word in an English and a Norwegian sentence, the two words are assumed to be cognates. For English/Norwegian a value of n=6 or 7 gives good results. Dice's similarity coefficient is the number of matching bigrams in the two words divided by the mean of the number of bigrams for the two words (2a/(b+c), where a is the matching number of bigrams, and b and c are the number of bigrams in the two words. For English and Norwegian, a value of more than 0.7 or 0.8 gives reasonable results. For other languages, the acceptable value for the coefficient can be less.''

Now, the question is how to find the coefficient. It is probably far smaller than for eng-nob.


!!!Preprocessing

The two languages entering the preprocessing procedure might be preprocessed 
according to different principles. The difference might be subtle: One common 
abbreviation oder initial letter classified differently in language A and language B
might be enough to eschew the result.

Investigate this.

* __Responsible__: Ciprian
* __Deadline__:



!!!Making a gold corpus for parameter tweaking

* __Responsible__: 
* __Deadline__:


